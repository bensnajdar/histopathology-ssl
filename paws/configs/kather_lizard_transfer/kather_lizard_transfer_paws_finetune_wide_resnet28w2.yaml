description: PAWS-Kather-Lizard-Transfer-Finetune
name: Kather-Lizard-Transfer::wide_resnet28w2::Split_Reduce # Split_Reduce, Stability_Experiment
labels:
  - wide_resnet28w2
  - finetune
  - kather-lizard-transfer
  - 20_Epochs
  - 1.0_Split
data:
  worker: 4
reproducibility:
  experiment_seed: 1643118217 # 1643118217 fixed seed for split reduce
hyperparameters:
  copy_data: false
  use_fp16: true
  method: paws
  # Data
  dataset: lizard
  number_classes: 4
  normalize_data: true
  train_augmentation: light_stack
  test_augmentation: plain
  img_rescale_size: 96
  pred_head_structure:
       type: categorical
       vals:
        - one_layer_mlp   # three_layer_mlp one_layer_mlp
        - three_layer_mlp
  split_size: 
      type: categorical
      vals:
      - 1.0 # (297245*0.08)/32 = 743 --> 20 epochs = 14860 , (297245*1.0)/256 = 1161 --> 20 epochs = 23220
  split_seed: 42
  # Encoder Evaluation
  plot_embedding_each: 1161 # 743, 1161 # 1Epoch
  # Model
  model_name: wide_resnet28w2 #base model
  dropout_rate: 
      type: categorical
      vals: 
      - 0.0
  use_pred_head: True
  output_dim: 128
  freeze_encoder: 
      type: categorical
      vals:
      - 2972 # 4 epochs on 0.08 split
      - true # if > max length its never trained
  checkpoint_uuid:
      type: categorical
      vals:
      # kather encoder
      - 54c83e3c-0886-47de-a3b7-b6f3fb90445f # 11218 - 81979  0.08, 1.0 Split
      #- c9796043-934d-4b96-8e5e-dd709d5a9d33 # 11218 - 81980 0.008 Split
      #- a7e7c067-bf59-490b-ab3e-2f4297c49c2d # 11218 - 81981  0.002 Split
      - null
      # for stability experiments
      #- 9327b7da-12a3-4cc7-bfcc-2bb9d8021151 # Seed 1 11557-84169
      #- 32bf46bc-0028-4d04-87eb-e042fed21a6e # Seed 2 11558-84170
      #- 975bcb73-41e0-43c5-bbb5-3fc91e3dd78a # Seed 3 11559-84171
      #- 37bb06be-1482-4ced-a56e-5da07144abb8 # Seed 4 11560-84172
      #- 6572a7a4-e584-4468-b306-e224bba8aca6 # Seed 5 11561-84173
  # -- optimizer
  momentum: 0.9
  lr: 
      type: categorical
      vals:
      - 0.005
  weight_decay: 1e-4
  scheduler: true
  use_larc: false
  # -- training 
  global_batch_size: 256 # 32, 256
# Single run without hyperparameter tuning
searcher:
  name: grid
  metric: v_acc
  smaller_is_better: false
  max_length:
    batches: 23220 # 20 Epochs for 0.08 Split with batch size 32 -> 14860, full supervised 23220
min_validation_period:
  batches: 1161 # 743, 1161 # 1Epoch
checkpoint_storage:
  save_trial_best: 1
  save_trial_latest: 1
  save_experiment_best: 0
resources:
  slots_per_trial: 1
  agent_label: dt-cluster #dt-cluster pepper-cluster
  max_slots: 4
max_restarts: 0
# Docker container used
environment:
  image:
    cpu: "deepprojects/determined-pytorch-1.10-cpu:1.0.0"
    gpu: "deepprojects/determined-cuda-113-pytorch-1.10-gpu:1.0.0"
# Bind Avocado into the docker container
bind_mounts:
 - host_path: "/data/determined/shared_fs/checkpoints"
   container_path: "/checkpoints"
   read_only: true
 - host_path: /data/ldap
   container_path: /data/ldap
   read_only: true
entrypoint: finetune_trial:Finetune